{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOADING & STORING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the files for all the data\n",
    "set1_all_before = pd.read_csv('set_1/AllDataBefore.csv')\n",
    "set1_all_after = pd.read_csv('set_1/AllDataAfter.csv')\n",
    "set2_all_before = pd.read_csv('set_2/AllDataBefore.csv')\n",
    "set2_all_after = pd.read_csv('set_2/AllDataAfter.csv')\n",
    "\n",
    "# defining the path to the folders\n",
    "set1_before_folder_path = '/Users/andrewcarranti/CODE/AI_CODE/SPRING/set_1/before/'\n",
    "set1_after_folder_path = '/Users/andrewcarranti/CODE/AI_CODE/SPRING/set_1/after/'\n",
    "set2_before_folder_path = '/Users/andrewcarranti/CODE/AI_CODE/SPRING/set_2/before/'\n",
    "set2_after_folder_path = '/Users/andrewcarranti/CODE/AI_CODE/SPRING/set_2/after/'\n",
    "\n",
    "# empty dictionary to store the data frames for each file in each set and their respective before & after\n",
    "set1_before_data_dict = {}\n",
    "set1_after_data_dict = {}\n",
    "set2_before_data_dict = {}\n",
    "set2_after_data_dict = {}\n",
    "folder_paths_dicts = [(set1_before_folder_path, set1_before_data_dict), (set1_after_folder_path, set1_after_data_dict), \n",
    "                      (set2_before_folder_path, set2_before_data_dict), (set2_after_folder_path, set2_after_data_dict)]\n",
    "\n",
    "# loop through the csv files in the folder\n",
    "for folder_path, data_dict in folder_paths_dicts:\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv') & filename.startswith('_') == False:\n",
    "            # read in the Excel file as a data frame\n",
    "            df = pd.read_csv(os.path.join(folder_path, filename))\n",
    "            \n",
    "            # store the data frame in the dictionary with the filename as the key\n",
    "            data_dict[filename] = df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set1 before & after: 192 & 192\n",
      "set2 before & after: 236 & 236\n"
     ]
    }
   ],
   "source": [
    "print(\"set1 before & after:\", len(set1_before_data_dict), \"&\", len(set1_after_data_dict))\n",
    "print(\"set2 before & after:\", len(set2_before_data_dict), \"&\", len(set2_after_data_dict))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty dictionary to store the results\n",
    "set1_decreased_before_dict = {}\n",
    "set1_decreased_after_dict = {}\n",
    "\n",
    "set1_increased_before_dict = {}\n",
    "set1_increased_after_dict = {}\n",
    "\n",
    "set2_decreased_before_dict = {}\n",
    "set2_decreased_after_dict = {}\n",
    "\n",
    "set2_increased_before_dict = {}\n",
    "set2_increased_after_dict = {}\n",
    "\n",
    "metrics_dicts = [(\"set1\", set1_decreased_before_dict, set1_decreased_after_dict, set1_increased_before_dict, set1_increased_after_dict, set1_before_data_dict, set1_after_data_dict), \n",
    "                (\"set2\", set2_decreased_before_dict, set2_decreased_after_dict, set2_increased_before_dict, set2_increased_after_dict, set2_before_data_dict, set2_after_data_dict)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IDENTIFY DECREASES (INCREASES) IN METRICS : IMPROVEMENT (DEGRADATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the two sets\n",
    "for set, decreased_before_metrics_dict, decreased_after_metrics_dict, increased_before_metrics_dict, increased_after_metrics_dict, before_data_dict, after_data_dict in metrics_dicts:\n",
    "\n",
    "    # iterate over the \"before\" data frames\n",
    "    for before_key, before_df in before_data_dict.items():\n",
    "\n",
    "        # get the corresponding \"after\" data frame\n",
    "        after_key = before_key.replace(\"_before.csv\", \"_after.csv\")\n",
    "        after_df = after_data_dict.get(after_key)\n",
    "\n",
    "        # filter the rows with \"Kind\" containing the word \"class\"\n",
    "        before_class_df = before_df[before_df['Kind'].str.contains(\"Class\")]\n",
    "        after_class_df = after_df[after_df['Kind'].str.contains(\"Class\")]\n",
    "\n",
    "        # skip the data frame if it has no classes\n",
    "        if len(before_df) == 0 or len(after_df) == 0:\n",
    "            continue        \n",
    "\n",
    "        # inner merge based on name\n",
    "        merged_df = pd.merge(before_class_df, after_class_df, on = 'Name', how = 'inner')\n",
    "\n",
    "        # skip the data frame if it has no common classes\n",
    "        if len(merged_df) == 0:\n",
    "            continue  \n",
    "\n",
    "        # get the columns that end with \"_x\" from the merged data frame\n",
    "        before_columns = [col for col in merged_df.columns if col.endswith(\"_x\")]\n",
    "        # get the columns that end with \"_y\" from the merged data frame\n",
    "        after_columns = [col for col in merged_df.columns if col.endswith(\"_y\")]\n",
    "\n",
    "        # ddd the \"Name\" column to both sets of columns\n",
    "        before_columns.insert(0, \"Name\")\n",
    "        after_columns.insert(0, \"Name\")\n",
    "\n",
    "        # separate the merged data frame into original data frames\n",
    "        before_class_df = merged_df[before_columns]\n",
    "        after_class_df = merged_df[after_columns]\n",
    "\n",
    "        # remove the \"_x\" at the end of each column name\n",
    "        before_class_df.columns = [col.rstrip(\"_x\") if col.endswith(\"_x\") else col for col in before_columns]\n",
    "        # remove the \"_y\" at the end of each column name\n",
    "        after_class_df.columns = [col.rstrip(\"_y\") if col.endswith(\"_y\") else col for col in after_columns]\n",
    "        \n",
    "        # drop the \"Kind\" and \"Name\" columns so can numerically compare\n",
    "        before_class_metrics_df = before_class_df.drop(columns=['Kind', 'Name'])\n",
    "        after_class_metrics_df = after_class_df.drop(columns=['Kind', 'Name'])\n",
    "\n",
    "        # empty list to store the rows with at least one decrease/increase in value\n",
    "        decreased_rows = []\n",
    "        increased_rows = []\n",
    "\n",
    "        # iterate over the rows in the data frames\n",
    "        for index, rowB in before_class_metrics_df.iterrows():\n",
    "            rowA = after_class_metrics_df.loc[index]\n",
    "\n",
    "            # check if any column has a decrease in value\n",
    "            if (np.sum(rowB > rowA) > len(rowB) / 2):\n",
    "                decreased_rows.append(index)\n",
    "\n",
    "            # check if any column has an incerase in value\n",
    "            if (np.sum(rowB < rowA) < len(rowB) / 2):\n",
    "                increased_rows.append(index)\n",
    "\n",
    "        # filter the original data frames based on decreased/increased indices\n",
    "        decreased_before_df = before_class_df.loc[decreased_rows]\n",
    "        increased_before_df = before_class_df.loc[increased_rows]\n",
    "        decreased_after_df = after_class_df.loc[decreased_rows]\n",
    "        increased_after_df = after_class_df.loc[increased_rows]\n",
    "\n",
    "    # add the file names & their corresponding data frame to the appropriate dictionary\n",
    "    if set == \"set1\":\n",
    "        if not decreased_before_df.empty:\n",
    "            set1_decreased_before_dict[before_key] = decreased_before_df\n",
    "            \n",
    "        if not decreased_after_df.empty:\n",
    "            set1_decreased_after_dict[after_key] = decreased_after_df\n",
    "\n",
    "        if not decreased_before_df.empty:\n",
    "            set1_increased_before_dict[before_key] = increased_before_df\n",
    "\n",
    "        if not decreased_after_df.empty:\n",
    "            set1_increased_after_dict[after_key] = increased_after_df\n",
    "        \n",
    "    elif set == \"set2\":\n",
    "        if not decreased_before_df.empty:\n",
    "            set2_decreased_before_dict[before_key] = decreased_before_df\n",
    "\n",
    "        if not decreased_after_df.empty:\n",
    "            set2_decreased_after_dict[after_key] = decreased_after_df\n",
    "\n",
    "        if not decreased_before_df.empty:\n",
    "            set2_increased_before_dict[before_key] = increased_before_df\n",
    "\n",
    "        if not decreased_after_df.empty:\n",
    "            set2_increased_after_dict[after_key] = decreased_after_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPORTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete!\n"
     ]
    }
   ],
   "source": [
    "dictionaries = [(set1_decreased_before_dict, \"set1_decreased_before_dict\"),\n",
    "                (set1_decreased_after_dict, \"set1_decreased_after_dict\"),\n",
    "                (set1_increased_before_dict, \"set1_increased_before_dict\"),\n",
    "                (set1_increased_after_dict, \"set1_increased_after_dict\"),\n",
    "                (set2_decreased_before_dict, \"set2_decreased_before_dict\"),\n",
    "                (set2_decreased_after_dict, \"set2_decreased_after_dict\"), \n",
    "                (set2_increased_before_dict, \"set2_increased_before_dict\"), \n",
    "                (set2_increased_after_dict, \"set2_increased_after_dict\")]\n",
    "\n",
    "# for each dictionary & folder name for the dictionary to contain the files\n",
    "for dictionary, name in dictionaries:\n",
    "\n",
    "    # for each file name (key) & dataframe (value) in the dictionary\n",
    "    for file_name, df in dictionary.items():\n",
    "        if not os.path.exists(name):\n",
    "            os.makedirs(name)\n",
    "            \n",
    "        # create file path\n",
    "        file_path = os.path.join(name, file_name)\n",
    "        \n",
    "        # export to csv\n",
    "        df.to_csv(file_path, index=False)  # Export DataFrame to CSV\n",
    "\n",
    "print(\"Export complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
